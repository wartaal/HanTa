{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to Use the Hanover Tagger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "* [Installation and Import](#sec-instal)\n",
    "* [German](#sec-german)\n",
    "* [Dutch](#sec-dutch)\n",
    "* [English](#sec-english)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation and Import<a class=\"anchor\" id=\"sec-installation\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: HanTa in c:\\users\\wartena\\anaconda3\\lib\\site-packages (0.2.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install HanTa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import HanoverTagger as ht"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## German<a class=\"anchor\" id=\"sec-german\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load a trained model. E.g. the model on Github trained on the TIGER-Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagger = ht.HanoverTagger('morphmodel_ger.pgz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing a word\n",
    "\n",
    "The method analyze gives the most probable part of speech, the lemma and a morphological analysis of a word.  By using the optional parameter taglevel, we can very the amount of information shown:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Fachmarkt', 'NN')\n",
      "NN\n",
      "('Fachmarkt', 'NN')\n",
      "('fach+markt+e', 'NN')\n",
      "('fachmarkt', [('fach', 'NN'), ('märkt', 'NN_VAR'), ('e', 'SUF_NN')], 'NN')\n"
     ]
    }
   ],
   "source": [
    "print(tagger.analyze('Fachmärkte'))\n",
    "print(tagger.analyze('Fachmärkte',taglevel=0))\n",
    "print(tagger.analyze('Fachmärkte',taglevel=1))\n",
    "print(tagger.analyze('Fachmärkte',taglevel=2))\n",
    "print(tagger.analyze('Fachmärkte',taglevel=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the taglevel is set to 1 the Hanover Tagger tries to generate the correct lemma. For the levels 2 and 3 the stem of te word is given."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('werfen', 'VV(FIN)')\n",
      "('werf+t', 'VV(FIN)')\n",
      "('werf', [('wirf', 'VV_VAR'), ('t', 'SUF_FIN')], 'VV(FIN)')\n"
     ]
    }
   ],
   "source": [
    "print(tagger.analyze('wirft',taglevel=1))\n",
    "print(tagger.analyze('wirft',taglevel=2))\n",
    "print(tagger.analyze('wirft',taglevel=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the parameter pos we can force to give the most likely analysis for the given part of speech."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('vertrau', [('vertrau', 'VVnp'), ('te', 'SUF_FIN')], 'VV(FIN)')\n",
      "('vertraut', [('vertrau', 'VVnp'), ('t', 'SUF_PP'), ('e', 'SUF_ADJ')], 'ADJ(D)')\n",
      "('vertraut', [('vertrau', 'VVnp'), ('t', 'SUF_PP'), ('e', 'SUF_ADJ')], 'NNA')\n"
     ]
    }
   ],
   "source": [
    "print(tagger.analyze('vertraute',taglevel=3,pos='VV(FIN)'))\n",
    "print(tagger.analyze('vertraute',taglevel=3,pos='ADJ(D)'))\n",
    "print(tagger.analyze('vertraute',taglevel=3,pos='NNA'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tagging a word\n",
    "\n",
    "With the method tag_word we can get the most probable POS-tags for a word:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('VV(FIN)', -12.311673325151059),\n",
       " ('ADJ(A)', -13.777650841629542),\n",
       " ('NNA', -16.371677520215265)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagger.tag_word('verdachte')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The numbers are the natural logarithm of the probability that the given POS produces the word as estimated by the underlying Hidden Markov Model. Here e.g. the probability that a finite verb is realized by the word 'verdachte' is $e^{-18.7} = 7.56 \\cdot 10^{-9}$.\n",
    "\n",
    "Using the Parameter cutoff we can get more or less results. Cutoff give the maximal difference of the logprob of the last result with the best result. The cutoff Parameter does not apply to frequent words with cached analyses! The aim of the cutoff is to exclude impossible analyses. Each cached analysis, however, has been obeserved and is possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('NN', -18.662410642261552)]\n",
      "[('NN', -18.662410642261552), ('VV(FIN)', -25.772053305619295), ('ADJ(A)', -27.799424476126276)]\n",
      "[('NN', -18.662410642261552), ('VV(FIN)', -25.772053305619295), ('ADJ(A)', -27.799424476126276), ('ADV', -34.24922543179404), ('ADJ(D)', -34.40659304587046), ('NNA', -35.26082374287396), ('VV(IMP)', -35.958731005154625)]\n"
     ]
    }
   ],
   "source": [
    "print(tagger.tag_word('verdachte',cutoff=0))\n",
    "print(tagger.tag_word('verdachte',cutoff=10))\n",
    "print(tagger.tag_word('verdachte',cutoff=20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the optional Parameter casesensitive is set to True (the default value) uppercase is used to guess the most likely part of speech, mainly favouring noun readings ove other possibilities. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('NNA', -12.0875), ('VV(FIN)', -12.3106), ('ADJ(A)', -13.6969)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagger.tag_word('Vertraute',casesensitive=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('NNA', -12.10138084948469),\n",
       " ('ADJ(A)', -16.25339063408611),\n",
       " ('VV(FIN)', -19.14813044593456)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagger.tag_word('Vertraute',casesensitive=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tagging a sentence\n",
    "\n",
    "The Hanover Tagger also can tag all words in a sentence at once. First probabilities for each word and POS are computed. Then a trigramm sentence model is used to disambiguate the tags and select the contextuall most approriates POS. Finally, the words are analysed again for the best POS and the analysis for each word is given. \n",
    "\n",
    "Here we can again use the parameters taglevel and casesensitive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Die', 'der', 'ART'),\n",
      " ('Europawahl', 'Europawahl', 'NN'),\n",
      " ('in', 'in', 'APPR'),\n",
      " ('den', 'der', 'ART'),\n",
      " ('Niederlanden', 'Niederlanden', 'NE'),\n",
      " ('findet', 'finden', 'VV(FIN)'),\n",
      " ('immer', 'immer', 'ADV'),\n",
      " ('donnerstags', 'donnerstags', 'ADV'),\n",
      " ('statt', 'statt', 'PTKVZ'),\n",
      " ('.', '.', '$.')]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from pprint import pprint\n",
    "\n",
    "sent = \"Die Europawahl in den Niederlanden findet immer donnerstags statt.\"\n",
    "\n",
    "words = nltk.word_tokenize(sent)\n",
    "lemmata = tagger.tag_sent(words)\n",
    "pprint(lemmata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Die', 'der', [('die', 'ART')], 'ART'),\n",
      " ('Sozialdemokraten',\n",
      "  'sozialdemokrat',\n",
      "  [('sozialdemokrat', 'NN'), ('en', 'SUF_NN')],\n",
      "  'NN'),\n",
      " ('haben', 'hab', [('hab', 'VA'), ('en', 'SUF_FIN')], 'VA(FIN)'),\n",
      " ('ersten', 'erst', [('erst', 'ADJ'), ('en', 'SUF_ADJ')], 'ADJ(A)'),\n",
      " ('Prognosen', 'prognose', [('prognose', 'NN'), ('n', 'SUF_NN')], 'NN'),\n",
      " ('zufolge', 'zufolge', [('zufolge', 'APPO')], 'APPO'),\n",
      " ('die', 'der', [('die', 'ART')], 'ART'),\n",
      " ('Europawahl', 'europawahl', [('europawahl', 'NN')], 'NN'),\n",
      " ('in', 'in', [('in', 'APPR')], 'APPR'),\n",
      " ('den', 'der', [('den', 'ART')], 'ART'),\n",
      " ('Niederlanden', 'niederlanden', [('niederlanden', 'NE')], 'NE'),\n",
      " ('gewonnen',\n",
      "  'gewinn',\n",
      "  [('gewonn', 'VVnp_VAR_PP'), ('en', 'SUF_PP')],\n",
      "  'VV(PP)'),\n",
      " ('.', '.', [('.', '$.')], '$.')]\n"
     ]
    }
   ],
   "source": [
    "sent = \"Die Sozialdemokraten haben ersten Prognosen zufolge die Europawahl in den Niederlanden gewonnen.\"\n",
    "\n",
    "words = nltk.word_tokenize(sent)\n",
    "lemmata = tagger.tag_sent(words,taglevel = 3)\n",
    "pprint(lemmata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ART', 'ADJ(A)', 'NN', 'NE', 'NE', 'VA(FIN)', 'ART', 'ADJ(A)', 'NN', 'APPRART', 'ADJ(A)', 'NN', '$,', 'PRELAT', 'NN', 'APPR', 'PIAT', 'NN', 'ADJ(A)', 'KON', 'ADJ(A)', 'NN', 'PTKVZ', 'VA(PP)', 'VA(FIN)', '$.']\n"
     ]
    }
   ],
   "source": [
    "sent = \"Der palästinensische Schriftsteller Emil Habibi ist der einzige Autor im Nahen Osten, dessen Werk von allen Seiten größte und offizielle Anerkennung zuteil geworden ist.\"\n",
    "\n",
    "words = nltk.word_tokenize(sent)\n",
    "tags = tagger.tag_sent(words,taglevel= 0)\n",
    "print(tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some information on the underlying tagging model\n",
    "\n",
    "The German model was trained on data derived from the Tiger Corpus. Hence the POS-tags are almost the same as in the Tiger Corpus, sc. the tags from the STuttgart Tübingen Tagset. See https://www.ims.uni-stuttgart.de/documents/ressourcen/korpora/tiger-corpus/annotation/tiger_scheme-morph.pdf (esp. pp 26/27). A general description of the tagset is available e.g. here: https://www.ims.uni-stuttgart.de/forschung/ressourcen/lexika/germantagsets/#id-cfcbf0a7-0 or here: https://homepage.ruhr-uni-bochum.de/stephen.berman/Korpuslinguistik/Tagsets-STTS.html\n",
    "\n",
    "The tags used for the morphemes are derived from the POS tags. \n",
    "\n",
    "HanTa can list all the POS tags and the tags for morphemes with some random examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$(\t..., :, ,, \", /, (, -, )\n",
      "$,\t,\n",
      "$.\t?, :, ;, ., !\n",
      "ADJ(A)\tjungen, europäische, sogenannte, große, moderne, britische, israelische, neue, bestimmten, dresdner\n",
      "ADJ(D)\talt, recht, lang, unterschiedlich, rasch, heftig, erheblich, offen, überwiegend, bekannt\n",
      "ADV\tzurück, bis, demnächst, knapp, zuletzt, zu, unten, gar, somit, sonst\n",
      "APPO\tungeachtet, gegenüber, über, voran, hinunter, zufolge, wegen, entlang, halber, entgegen\n",
      "APPR\tanno, voller, via, seit, mitsamt, inmitten, a, minus, gemäß, infolge\n",
      "APPRART\tübers, vorm, v., im, unters, beim, z., zum, a., überm\n",
      "APZR\theraus, willen, aus, herunter, her, hinaus, herum, hinein, hinweg, hin\n",
      "ART\tdas, 'n, det, einem, d., einen, eines, die, einer, s\n",
      "CARD\t75, 700, 2,5, 2500, 1986, 1987, 29, 3,5, 400, 24\n",
      "FM\tbritish, friends, il, 's, &, labour, news, sir, spe, la\n",
      "ITJ\tmann, o, na, ach, ja\n",
      "KOKOM\twie, als, denn\n",
      "KON\tplus, ebenso, +, mal, weder, sondern, u, wenngleich, entweder, beziehungsweise\n",
      "KOUI\tstatt, ums, anstatt, ohne, um\n",
      "KOUS\tehe, sofern, wie, zumal, obwohl, daß, obgleich, bis, ob, denn\n",
      "NE\tdüsseldorf, bayern, bremen, europa, schmidt, hamburg, walter, tel, franz, york\n",
      "NN\tplatz, kollegen, art, mitte, außenminister, kritik, beginn, rolle, medien, republik\n",
      "NNA\tillegalen, englischen, vorsitzender, rechtsradikalen, armen, studierende, lateinischen, beschäftigter, kranken, beschuldigte\n",
      "PDAT\tjenen, dieselbe, desselben, solche, jener, demselben, diese, jenem, dieses, dieselben\n",
      "PDS\tjenes, denselben, eine, derjenigen, dieselben, jene, diesen, jenen, der, diejenigen\n",
      "PIAT\tmehr, mehrerer, wenigen, soviel, jeder, meisten, wenigsten, nichts, eine, bißchen\n",
      "PIS\twelche, jeden, solches, bißchen, jedes, vieler, einzigen, irgendeine, meiste, etwas\n",
      "PPER\twir, ihr, ihn, 's, ick, ihm, mir, du, dich, uns\n",
      "PPOSAT\tsein, meinen, unsere, eurer, eure, meine, unseren, seine, meinem, seiner\n",
      "PPOSS\tmeiner, seine, seines, seinem, seinen, ihren, unseren\n",
      "PRELAT\tdessen, deren\n",
      "PRELS\twelchem, demzufolge, was, denen, derer, der, die, das, welcher, dem\n",
      "PRF\tmir, dich, einander, sich, euch, mich, uns\n",
      "PROAV\tworüber, hierzu, zudem, währenddessen, deswegen, hinzu, nacheinander, dabei, darum, hierbei\n",
      "PTKA\tzu, allzu, am\n",
      "PTKANT\tgewiß, bitte, ja, nein\n",
      "PTKNEG\tnicht\n",
      "PTKVZ\tfort, durcheinander, um, bloß, kalt, dabei, gefangen, abwärts, stark, dran\n",
      "PTKZU\tzum, zur, zu\n",
      "PWAT\twelchem, welchen, welches, wieviel, welch, wieviele, welcher, welche\n",
      "PWAV\tworaufhin, wovon, wohin, was, wogegen, wobei, wieviel, woraus, inwieweit, womit\n",
      "PWS\twen, welches, was, wer, wem, welcher, wieviele, welche, wieviel\n",
      "TRUNC\taußen-, innen-, bio-, regierungs-, obst-, chemie-, energie-, sach-, tier-, landes-\n",
      "UNKNOWN\t\n",
      "VA(FIN)\thabe, sei, hat, hätten, werden, wäre, hätte, würden, wurden, hast\n",
      "VA(IMP)\tsei, seid, werde\n",
      "VA(INF)\tworden, werden, haben, sein\n",
      "VA(PP)\tgehabt, gewesen, geworden, worden\n",
      "VM(FIN)\tmochte, mag, dürfen, wollte, müssen, solle, wollten, muß, sollten, wollt\n",
      "VM(INF)\tdürfen, können, wollen, sollen, müssen\n",
      "VM(PP)\tgewollt\n",
      "VV(FIN)\tgalt, teilte, brachte, weiß, nennt, steht, kritisierte, sehen, meldete, stimmt\n",
      "VV(IMP)\tbrecht, pflegt, erspart, wartet, kommt, s., versucht, halt, denke, gehe\n",
      "VV(INF)\tversuchen, unterstützen, bewegen, wissen, erfüllen, liegen, bringen, verdanken, beschäftigen, ändern\n",
      "VV(IZU)\teinzugreifen, aufzulösen, hinzunehmen, aufzuklären, vorzunehmen, aufzubessern, aufzugreifen, einzuführen, auszugrenzen, abzulegen\n",
      "VV(PP)\tvereinbart, erhalten, festgestellt, gestiegen, erworben, gespielt, gefolgt, gesetzt, bestimmt, gefordert\n",
      "XY\tba, ap, vs, ug, &bullet;, geg, kp, u, wb, wal\n"
     ]
    }
   ],
   "source": [
    "tagger.list_postags()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ADJ             technisch, jüdisch, 13., allgemein, hiesig, 14., 19., zahlreich, schwierig, tief\n",
      "ADJ_COMP        er\n",
      "ADJ_IRR         täglich, größtes, viertgrößte, wahrscheinlich, politisch, größter, unklar, letzter, größtem, frühren\n",
      "ADJ_SUP         st, est\n",
      "ADJ_VAR         abgeschirmt, treuest, edl, bess, betreut, geschmiegt, geknüpft, schwach, bekanntgegeben, irreversibl\n",
      "FUGE            en, es, er, e, nen, -, s, n\n",
      "HYPHEN          -\n",
      "NE_VAR          courmayeur, l, karlsruhe, skipi, shakespear, düsseldorf, jacques, lersnerstr., mehrdorn, saigon-fluss\n",
      "NN_IRR          -bedingungen, -fenster, ``aldi''-brüder, -steuer, ``klassik''-mitarbeiter, ``rations''-abschnitten, ``kavadi''-träger, öfen, -führer, -desaster\n",
      "NN_VAR          rückgäng, abläuf, anträg, verbänd, verstöß, wäld, töpf, nöt, sprüch, anschläg\n",
      "PDAT_VAR        solch, jen, dieselbe, derartig, dasselbe, ebendies, di, denjenige, desselbe, demselbe\n",
      "PIS_IRR         ihresgleichen\n",
      "PIS_VAR         wat, ein, wen'g, viel\n",
      "PPOSAT_VAR      ihr, eurer, eur\n",
      "PREF_PP         ge\n",
      "PRESPART        end, nd\n",
      "PTKVZ_DUBIUM    um, wieder, über, unter, durch\n",
      "PTKVZ_SEP       zutage, vorüber, nach, kaputt, preis, , fern, stehen, fertig, hervor\n",
      "SUF_ADJ         er, ere, es, e, en, em, erer, eres\n",
      "SUF_DERIV_ADJ   er\n",
      "SUF_FIN         n, ten, ete, eten, en's, est, t, (te), en, d\n",
      "SUF_IMP         t, e, et\n",
      "SUF_INF         n, 'n, en\n",
      "SUF_IZU         n, en\n",
      "SUF_NE          s, n, \", is, -, es, e, 's, en, ist\n",
      "SUF_NN          a, ste, ses, r, ien, in, g, ens, ns, er\n",
      "SUF_PDAT        em, e, n, en, es\n",
      "SUF_PIAT        e, l, r, es, er, n, em, en\n",
      "SUF_PIS         en, n, er, e, em, m, s, r, ie, es\n",
      "SUF_PP          et, ete, t, en, te, end, n\n",
      "SUF_PPOSAT      em, n, ers, es, m, en, er, e\n",
      "VA              werd, word, sei, dabeisei, hab\n",
      "VA_IRR          sind, ist's, gewesen, wart, war, sei's, wär's, war's, bist, wär\n",
      "VA_VAR          hätt', hatt, hat's, wir, wurd, ha, würd, wird's, hätt, bü\n",
      "VA_VAR_PP       word\n",
      "VM              könn, möchte, möcht, woll, mög, müss, möch, dürf, soll\n",
      "VM_VAR          müßt, läß, mocht, musst, darf, will, vermißt, kann, muß, mußt\n",
      "VM_VAR_PP       moch, konn\n",
      "VV              deut, schrumpf, steck, setz, hör, schaff, such, plan, zahl, anschließ\n",
      "VV_IRR          ausgetimed\n",
      "VV_VAR          würd, bot, schoß, rieb, schwieg, trat, lief, wäsch, zwang, blieb\n",
      "VV_VAR_PP       l, vollbrach, schrieb, stoss, rann, haß, unterworf, wand, unternomm, troff\n",
      "VVnp            bedroh, beginn, appellier, bezieh, erläuter, erfüll, entfern, bezahl, begleit, verabschied\n",
      "VVnp_VAR        behält, bestand, berief, erlit, verschläg, gerät, ergäb, ertrank, entsandt, erklang\n",
      "VVnp_VAR_PP     errung, gewonn, bedurf, verweb, befürtwort, übergegriff, bestand, hinterhergelauf, begang, beglich\n"
     ]
    }
   ],
   "source": [
    "tagger.list_mtags()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dutch<a class=\"anchor\" id=\"sec-dutch\"></a>\n",
    "\n",
    "You can load trained morphology models for some other languages in the same way as shown above for German. Here a few examples for a Dutch model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagger_nl = ht.HanoverTagger('morphmodel_dutch.pgz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('huishoudhulp', 'N(soort,ev,dim,onz,stan)')\n",
      "N(soort,ev,dim,onz,stan)\n",
      "('huishoudhulp', 'N(soort,ev,dim,onz,stan)')\n",
      "('huis+houd+hulp+je', 'N(soort,ev,dim,onz,stan)')\n",
      "('huishoudhulp', [('huis', 'N(soort,onz)'), ('houd', 'WW'), ('hulp', 'N(soort,zijd)'), ('je', 'SUF_DIM')], 'N(soort,ev,dim,onz,stan)')\n"
     ]
    }
   ],
   "source": [
    "print(tagger_nl.analyze('huishoudhulpje'))\n",
    "print(tagger_nl.analyze('huishoudhulpje',taglevel=0))\n",
    "print(tagger_nl.analyze('huishoudhulpje',taglevel=1))\n",
    "print(tagger_nl.analyze('huishoudhulpje',taglevel=2))\n",
    "print(tagger_nl.analyze('huishoudhulpje',taglevel=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('N(soort,ev,basis,onz,stan)', -9.735243330616047),\n",
       " ('WW(inf,vrij,zonder)', -12.609773795398848),\n",
       " ('WW(pv,tgw,mv)', -13.014584292537286)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagger_nl.tag_word('vertrouwen')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Elk',\n",
      "  'elk',\n",
      "  [('elk', 'VNW(onbep,det,stan,prenom,zonder,evon)')],\n",
      "  'VNW(onbep,det,stan,prenom,zonder,evon)'),\n",
      " ('jaar', 'jaar', [('jaar', 'N(soort,onz)')], 'N(soort,ev,basis,onz,stan)'),\n",
      " ('wisselen',\n",
      "  'wissel',\n",
      "  [('wissel', 'WW'), ('en', 'SUF_WW(mv)')],\n",
      "  'WW(pv,tgw,mv)'),\n",
      " ('ruim', 'ruim', [('ruim', 'ADJ')], 'ADJ(vrij,basis,zonder)'),\n",
      " ('1', '1', [('1', 'TW(hoofd,prenom,stan)')], 'TW(hoofd,prenom,stan)'),\n",
      " ('miljoen',\n",
      "  'miljoen',\n",
      "  [('miljoen', 'N(soort,onz)')],\n",
      "  'N(soort,ev,basis,onz,stan)'),\n",
      " ('Nederlanders',\n",
      "  'nederlander',\n",
      "  [('nederlander', 'N(eigen,zijd)'), ('s', 'SUF_N_S')],\n",
      "  'N(eigen,mv,basis)'),\n",
      " ('van', 'van', [('van', 'VZ(init)')], 'VZ(init)'),\n",
      " ('zorgverzekeraar',\n",
      "  'zorgverzekeraar',\n",
      "  [('zorg', 'N(soort,zijd)'), ('verzekeraar', 'N(soort,zijd)')],\n",
      "  'N(soort,ev,basis,zijd,stan)'),\n",
      " ('.', '.', [('.', 'LET()')], 'LET()')]\n"
     ]
    }
   ],
   "source": [
    "sent = \"Elk jaar wisselen ruim 1 miljoen Nederlanders van zorgverzekeraar. \"\n",
    "\n",
    "words = nltk.word_tokenize(sent)\n",
    "lemmata = tagger_nl.tag_sent(words,taglevel= 3)\n",
    "pprint(lemmata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## English<a class=\"anchor\" id=\"sec-english\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagger_en = ht.HanoverTagger('morphmodel_en.pgz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VBG\n",
      "('walk', 'VBG')\n",
      "('walk+ing', 'VBG')\n",
      "('walk', [('walk', 'VB'), ('ing', 'SUF_ING')], 'VBG')\n"
     ]
    }
   ],
   "source": [
    "print(tagger_en.analyze('walking',taglevel=0))\n",
    "print(tagger_en.analyze('walking',taglevel=1))\n",
    "print(tagger_en.analyze('walking',taglevel=2))\n",
    "print(tagger_en.analyze('walking',taglevel=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('VBZ', -12.031221308520081), ('NNS', -12.225015720529305)]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagger_en.tag_word('walks')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you analyze English sentences, make sure that the word tokenization is done properly. The model provided works with the default word tokenization from NLTK, which splits words like _cannot_ and _don't_ . If the wrong type of apostrope is used, tokenization might not gove the expected results, as is the case i the first variant of the following sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['VBG', 'AT', 'JJ', 'NN', 'MD', 'BE', 'AT', 'JJ', 'NN', ',', 'QL', 'RB', 'BEZ', 'AT', 'JJ', 'NN', 'IN', 'NNS', 'TO', 'VB', ',', 'NNS', 'IN', 'NN', ',', 'NN', 'TO', 'VB', 'CC', 'AP', '.']\n",
      "----\n",
      "[('Tackling', 'tackl', 'VBG'), ('the', 'the', 'AT'), ('entire', 'entire', 'JJ'), ('kitchen', 'kitchen', 'NN'), ('can', 'can', 'MD'), ('be', 'be', 'BE'), ('an', 'a', 'AT'), ('intimidating', 'intimidating', 'JJ'), ('task', 'task', 'NN'), (',', ',', ','), ('so', 'so', 'QL'), ('here', 'here', 'RB'), (\"'s\", \"'s\", 'BEZ'), ('a', 'a', 'AT'), ('manageable', 'manageable', 'JJ'), ('list', 'list', 'NN'), ('of', 'of', 'IN'), ('things', 'thing', 'NNS'), ('to', 'to', 'TO'), ('clean', 'clean', 'VB'), (',', ',', ','), ('ingredients', 'ingredient', 'NNS'), ('to', 'to', 'IN'), ('check', 'check', 'NN'), (',', ',', ','), ('equipment', 'equipment', 'NN'), ('to', 'to', 'TO'), ('organize', 'organize', 'VB'), ('and', 'and', 'CC'), ('more', 'more', 'AP'), ('.', '.', '.')]\n",
      "----\n",
      "[('Tackling', 'tackl+ing', 'VBG'), ('the', 'the', 'AT'), ('entire', 'entire', 'JJ'), ('kitchen', 'kitchen', 'NN'), ('can', 'can', 'MD'), ('be', 'be', 'BE'), ('an', 'a', 'AT'), ('intimidating', 'intimidating', 'JJ'), ('task', 'task', 'NN'), (',', ',', ','), ('so', 'so', 'QL'), ('here', 'here', 'RB'), (\"'s\", \"'s\", 'BEZ'), ('a', 'a', 'AT'), ('manageable', 'manage+able', 'JJ'), ('list', 'list', 'NN'), ('of', 'of', 'IN'), ('things', 'thing+s', 'NNS'), ('to', 'to', 'TO'), ('clean', 'clean', 'VB'), (',', ',', ','), ('ingredients', 'ingredient+s', 'NNS'), ('to', 'to', 'IN'), ('check', 'check', 'NN'), (',', ',', ','), ('equipment', 'equipment', 'NN'), ('to', 'to', 'TO'), ('organize', 'organize', 'VB'), ('and', 'and', 'CC'), ('more', 'more', 'AP'), ('.', '.', '.')]\n",
      "----\n",
      "[('Tackling', 'tackl', [('tackl', 'VB'), ('ing', 'SUF_ING')], 'VBG'), ('the', 'the', [('the', 'AT')], 'AT'), ('entire', 'entire', [('entire', 'JJ')], 'JJ'), ('kitchen', 'kitchen', [('kitchen', 'NN')], 'NN'), ('can', 'can', [('can', 'MD')], 'MD'), ('be', 'be', [('be', 'BE')], 'BE'), ('an', 'a', [('an', 'AT_VAR')], 'AT'), ('intimidating', 'intimidating', [('intimidating', 'JJ')], 'JJ'), ('task', 'task', [('task', 'NN')], 'NN'), (',', ',', [(',', ',')], ','), ('so', 'so', [('so', 'QL')], 'QL'), ('here', 'here', [('here', 'RB')], 'RB'), (\"'s\", \"'s\", [(\"'s\", 'BEZ')], 'BEZ'), ('a', 'a', [('a', 'AT')], 'AT'), ('manageable', 'manageable', [('manage', 'VB'), ('able', 'SUF_VBJJ')], 'JJ'), ('list', 'list', [('list', 'NN')], 'NN'), ('of', 'of', [('of', 'IN')], 'IN'), ('things', 'thing', [('thing', 'NN'), ('s', 'SUF_NN_S')], 'NNS'), ('to', 'to', [('to', 'TO')], 'TO'), ('clean', 'clean', [('clean', 'VB')], 'VB'), (',', ',', [(',', ',')], ','), ('ingredients', 'ingredient', [('ingredient', 'NN'), ('s', 'SUF_NN_S')], 'NNS'), ('to', 'to', [('to', 'IN')], 'IN'), ('check', 'check', [('check', 'NN')], 'NN'), (',', ',', [(',', ',')], ','), ('equipment', 'equipment', [('equipment', 'NN')], 'NN'), ('to', 'to', [('to', 'TO')], 'TO'), ('organize', 'organize', [('organize', 'VB')], 'VB'), ('and', 'and', [('and', 'CC')], 'CC'), ('more', 'more', [('more', 'AP')], 'AP'), ('.', '.', [('.', '.')], '.')]\n"
     ]
    }
   ],
   "source": [
    "#sent = \"Tackling the entire kitchen can be an intimidating task, so here’s a manageable list of things to clean, ingredients to check, equipment to organize and more.\"\n",
    "sent = \"Tackling the entire kitchen can be an intimidating task, so here's a manageable list of things to clean, ingredients to check, equipment to organize and more.\"\n",
    "\n",
    "words = nltk.word_tokenize(sent)\n",
    "\n",
    "print(tagger_en.tag_sent(words,taglevel = 0))\n",
    "print('----')\n",
    "print(tagger_en.tag_sent(words,taglevel = 1))\n",
    "print('----')\n",
    "print(tagger_en.tag_sent(words,taglevel = 2))\n",
    "print('----')\n",
    "print(tagger_en.tag_sent(words,taglevel = 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
