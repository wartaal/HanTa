{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to Use the Hanover Tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: HanTa in c:\\users\\wartena\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (0.1.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install HanTa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from HanTa import HanoverTagger as ht"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load a trained model. E.g. the model on Github trained on the TIGER-Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagger = ht.HanoverTagger('morphmodel_ger.pgz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing a word\n",
    "\n",
    "The method analyze gives the most probable part of speech, the lemma and a morphological analysis of a word.  Using the optional parameter taglevel, we can very the amount of information shown:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Fachmarkt', 'NN')\n",
      "NN\n",
      "('Fachmarkt', 'NN')\n",
      "('fach+markt', 'NN')\n",
      "('fachmarkt', [('fach', 'NN'), ('märkt', 'NN_VAR'), ('e', 'SUF_NN')], 'NN')\n"
     ]
    }
   ],
   "source": [
    "print(tagger.analyze('Fachmärkte'))\n",
    "print(tagger.analyze('Fachmärkte',taglevel=0))\n",
    "print(tagger.analyze('Fachmärkte',taglevel=1))\n",
    "print(tagger.analyze('Fachmärkte',taglevel=2))\n",
    "print(tagger.analyze('Fachmärkte',taglevel=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the taglevel is set to 1 the Hanover Tagger tries to generate the correct lemma. For the levels 2 and 3 the stem of te word is given."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('werfen', 'VVFIN')\n",
      "('werf', 'VVFIN')\n",
      "('werf', [('wirf', 'VV_VAR'), ('t', 'SUF_FIN')], 'VVFIN')\n"
     ]
    }
   ],
   "source": [
    "print(tagger.analyze('wirft',taglevel=1))\n",
    "print(tagger.analyze('wirft',taglevel=2))\n",
    "print(tagger.analyze('wirft',taglevel=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the parameter pos we can force to give the most likely analysis for the given part of speech."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('vertrau', [('ver', 'PREF_V'), ('trau', 'VV'), ('te', 'SUF_FIN')], 'VVFIN')\n",
      "('vertrau', [('ver', 'PREF_V'), ('trau', 'VV'), ('te', 'SUF_PP')], 'VVPP')\n",
      "('vertraut', [('ver', 'PREF_V'), ('trau', 'VV'), ('t', 'SUF_PP'), ('e', 'SUF_ADJ')], 'ADJA')\n",
      "('vertraute', [('vertraute', 'NN')], 'NN')\n"
     ]
    }
   ],
   "source": [
    "print(tagger.analyze('vertraute',taglevel=3,pos='VVFIN'))\n",
    "print(tagger.analyze('vertraute',taglevel=3,pos='VVPP'))\n",
    "print(tagger.analyze('vertraute',taglevel=3,pos='ADJA'))\n",
    "print(tagger.analyze('vertraute',taglevel=3,pos='NN'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tagging a word\n",
    "\n",
    "With the method tag_word we can get the most probable POS-tags for a word:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('VVFIN', -16.45041338404915),\n",
       " ('ADJA', -17.756739025841874),\n",
       " ('NN', -18.556571435108527)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagger.tag_word('verdachte')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The numbers are the natural logarithm of the probability that the given POS produces the word as estimated by the underlying Hidden Markov Model. Here e.g. the probability that a finite verb is realized by the word 'verdachte' is $e^{-16.5} = 7.18 \\cdot 10^{-8}$.\n",
    "\n",
    "Using the Parameter cutoff we can get more or less results. Cutoff give the maximal difference of the logprob of the last result with the best result. The cutoff Parameter does not apply to frequent words with cached analyses! The aim of the cutoff is to exclude impossible analyses. Each cached analysis, however, has been obeserved and is possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('VVFIN', -16.45041338404915)]\n",
      "[('VVFIN', -16.45041338404915), ('ADJA', -17.756739025841874), ('NN', -18.556571435108527)]\n",
      "[('VVFIN', -16.45041338404915), ('ADJA', -17.756739025841874), ('NN', -18.556571435108527), ('VVPP', -22.014090845844397), ('ADJD', -24.93130401111237), ('ADV', -27.35113055669061), ('VVIMP', -36.235118881764656)]\n"
     ]
    }
   ],
   "source": [
    "print(tagger.tag_word('verdachte',cutoff=0))\n",
    "print(tagger.tag_word('verdachte',cutoff=5))\n",
    "print(tagger.tag_word('verdachte',cutoff=20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the optional Parameter casesensitive is set to True (the default value) uppercase is used to guess the most likely part of speech, mainly favouring noun readings ove other possibilities. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('NN', -12.901501651584534), ('ADJA', -20.232237126454365)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagger.tag_word('Verdachte',casesensitive=True,cutoff=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('NN', -12.898008032529743),\n",
       " ('VVFIN', -16.449339998261664),\n",
       " ('ADJA', -17.675969432512957)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagger.tag_word('Verdachte',casesensitive=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('VVFIN', -16.45041338404915),\n",
       " ('ADJA', -17.756739025841874),\n",
       " ('NN', -18.556571435108527)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagger.tag_word('verdachte',casesensitive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('NN', -12.898008032529743),\n",
       " ('VVFIN', -16.449339998261664),\n",
       " ('ADJA', -17.675969432512957)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagger.tag_word('verdachte',casesensitive=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing sentences\n",
    "\n",
    "The Hanover Tagger also can analyse a whole sentence at once. First probabilities for each word and POS are computed. Then a trigramm sentence model is used to disambiguate the tags and select the contextuall most approriates POS. Finally, the words are analysed again for the best POS and the analysis for each word is given. \n",
    "\n",
    "Here we can again use the parameters taglevel and casesensitive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Die', 'die', 'ART'),\n",
      " ('Europawahl', 'Europawahl', 'NN'),\n",
      " ('in', 'in', 'APPR'),\n",
      " ('den', 'den', 'ART'),\n",
      " ('Niederlanden', 'Niederlanden', 'NE'),\n",
      " ('findet', 'finden', 'VVFIN'),\n",
      " ('immer', 'immer', 'ADV'),\n",
      " ('donnerstags', 'donnerstags', 'ADV'),\n",
      " ('statt', 'statt', 'PTKVZ'),\n",
      " ('.', '--', '$.')]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from pprint import pprint\n",
    "\n",
    "sent = \"Die Europawahl in den Niederlanden findet immer donnerstags statt.\"\n",
    "\n",
    "words = nltk.word_tokenize(sent)\n",
    "lemmata = tagger.tag_sent(words,taglevel= 1)\n",
    "pprint(lemmata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Die', 'die', [('die', 'ART')], 'ART'),\n",
      " ('Sozialdemokraten',\n",
      "  'sozialdemokrat',\n",
      "  [('sozial', 'NN'), ('demokrat', 'NN'), ('en', 'SUF_NN')],\n",
      "  'NN'),\n",
      " ('haben', 'hab', [('hab', 'VA'), ('en', 'SUF_FIN')], 'VAFIN'),\n",
      " ('ersten', 'erster', [('erst', 'ADJ'), ('en', 'SUF_ADJ')], 'ADJA'),\n",
      " ('Prognosen', 'prognose', [('prognose', 'NN'), ('n', 'SUF_NN')], 'NN'),\n",
      " ('zufolge', 'zufolge', [('zufolge', 'APPO')], 'APPO'),\n",
      " ('die', 'die', [('die', 'ART')], 'ART'),\n",
      " ('Europawahl', 'europawahl', [('europawahl', 'NN')], 'NN'),\n",
      " ('in', 'in', [('in', 'APPR')], 'APPR'),\n",
      " ('den', 'den', [('den', 'ART')], 'ART'),\n",
      " ('Niederlanden', 'niederlanden', [('niederlanden', 'NE')], 'NE'),\n",
      " ('gewonnen',\n",
      "  'gewinn',\n",
      "  [('ge', 'PREF_PP'), ('wonn', 'VV_VAR_PP'), ('en', 'SUF_PP')],\n",
      "  'VVPP'),\n",
      " ('.', '.', [('.', '$.')], '$.')]\n"
     ]
    }
   ],
   "source": [
    "sent = \"Die Sozialdemokraten haben ersten Prognosen zufolge die Europawahl in den Niederlanden gewonnen.\"\n",
    "\n",
    "words = nltk.word_tokenize(sent)\n",
    "lemmata = tagger.tag_sent(words,taglevel = 3)\n",
    "pprint(lemmata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ART', 'ADJA', 'NN', 'NE', 'NE', 'VAFIN', 'ART', 'ADJA', 'NN', 'APPRART', 'ADJA', 'NN', '$,', 'PRELAT', 'NN', 'APPR', 'PIAT', 'NN', 'ADJA', 'KON', 'ADJA', 'NN', 'PTKVZ', 'VAPP', 'VAFIN', '$.']\n"
     ]
    }
   ],
   "source": [
    "sent = \"Der palästinensische Schriftsteller Emil Habibi ist der einzige Autor im Nahen Osten, dessen Werk von allen Seiten größte und offizielle Anerkennung zuteil geworden ist.\"\n",
    "\n",
    "words = nltk.word_tokenize(sent)\n",
    "tags = tagger.tag_sent(words,taglevel= 0)\n",
    "print(tags)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
