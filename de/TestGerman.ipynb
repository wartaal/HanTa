{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests for HanTa German\n",
    "\n",
    "Some simple tests to check whether everything is doing what it is suposed to do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(1, '..')\n",
    "import HanoverTagger as ht #Do not import form the package but from the parent folder where the latest source file is found\n",
    "\n",
    "tagger = ht.HanoverTagger(r'../morphmodel_ger.pgz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A number of arbitrary examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Überlebende', 'NNA')\n",
      "('überlebend', 'ADJ(A)')\n",
      "('überlebend', [('überleb', 'VVnp'), ('end', 'PRESPART'), ('e', 'SUF_ADJ')], 'NNA')\n",
      "('überlebend', [('überleb', 'VVnp'), ('end', 'PRESPART'), ('e', 'SUF_ADJ')], 'ADJ(A)')\n"
     ]
    }
   ],
   "source": [
    "print(tagger.analyze('Überlebende'))\n",
    "print(tagger.analyze('überlebende'))\n",
    "print(tagger.analyze('Überlebende',taglevel=3))\n",
    "print(tagger.analyze('überlebende',taglevel=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('wohnzimmerschrank', [('wohn', 'VV'), ('zimmer', 'NN'), ('schränk', 'NN_VAR'), ('e', 'SUF_NN')], 'NN')\n",
      "('holzfußboden', [('holz', 'NN'), ('fuß', 'NN'), ('boden', 'NN')], 'NN')\n",
      "('schneeballsystem', [('schnee', 'NN'), ('ball', 'NN'), ('system', 'NN')], 'NN')\n",
      "('exportstandort', [('export', 'NN'), ('stand', 'NN'), ('ort', 'NN'), ('s', 'SUF_NN')], 'NN')\n"
     ]
    }
   ],
   "source": [
    "print(tagger.analyze('Wohnzimmerschränke',taglevel=3))\n",
    "print(tagger.analyze('Holzfußboden',taglevel=3))\n",
    "print(tagger.analyze('Schneeballsystem',taglevel=3))\n",
    "print(tagger.analyze('Exportstandorts',taglevel=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('NN', -18.04955148891443)]\n",
      "[('VV(INF)', -13.004997963605383), ('VV(PP)', -13.697760906300482), ('NN', -14.599291493868266)]\n",
      "[('NN', -8.871850108561302), ('VV(INF)', -19.731531081236866), ('VV(PP)', -20.754855308112653)]\n"
     ]
    }
   ],
   "source": [
    "print(tagger.tag_word('verdachten'))\n",
    "print(tagger.tag_word('verfahren'))\n",
    "print(tagger.tag_word('Verfahren'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('NN', -18.04955148891443), ('VV(FIN)', -25.368873598068934), ('ADJ(A)', -27.89646822627671)]\n"
     ]
    }
   ],
   "source": [
    "print(tagger.tag_word('verdachten',cutoff=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('zoom', [('zoom', 'NE')], 'NE')\n",
      "('lehrplan', [('lehr', 'NN_VAR_EL'), ('plan', 'NN')], 'NN')\n",
      "('lehrplan', [('lehr', 'NN_VAR_EL'), ('plän', 'NN_VAR'), ('e', 'SUF_NN')], 'NN')\n",
      "('landbewirtschaftung', [('landbewirtschaftung', 'NN')], 'NN')\n"
     ]
    }
   ],
   "source": [
    "print(tagger.analyze('Zoom',taglevel=3))\n",
    "print(tagger.analyze('Lehrplan',taglevel=3))\n",
    "print(tagger.analyze('Lehrpläne',taglevel=3))\n",
    "print(tagger.analyze('Landbewirtschaftung',taglevel=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('teilchen', [('teil', 'NN'), ('chen', 'SUF_DIM')], 'NN')\n",
      "('holzhäuschen', [('holz', 'NN'), ('häus', 'NN_VAR'), ('chen', 'SUF_DIM')], 'NN')\n",
      "('bundestagspräsidentin', [('bundestag', 'NN'), ('s', 'FUGE'), ('präsident', 'NN'), ('in', 'SUF_FEM')], 'NN')\n",
      "('lehrerin', [('lehrer', 'NN'), ('in', 'SUF_FEM')], 'NN')\n",
      "('freundin', [('freund', 'NN'), ('in', 'SUF_FEM')], 'NN')\n",
      "('kollegin', [('kolleg', 'NN_VAR'), ('in', 'SUF_FEM')], 'NN')\n",
      "('lehrerin', [('lehrer', 'NN'), ('in', 'SUF_FEM'), ('nen', 'SUF_NN')], 'NN')\n",
      "('freundin', [('freund', 'NN'), ('in', 'SUF_FEM'), ('nen', 'SUF_NN')], 'NN')\n",
      "('genossin', [('genoss', 'NN_VAR'), ('in', 'SUF_FEM'), ('nen', 'SUF_NN')], 'NN')\n",
      "('ärztin', [('ärzt', 'NN_VAR'), ('in', 'SUF_FEM'), ('nen', 'SUF_NN')], 'NN')\n",
      "('fdp-ehrenvorsitzend', [('fdp-ehrenvorsitzend', 'ADJ'), ('e', 'SUF_ADJ')], 'ADJ(A)')\n"
     ]
    }
   ],
   "source": [
    "print(tagger.analyze('Teilchen',taglevel=3))\n",
    "print(tagger.analyze('Holzhäuschen',taglevel=3))\n",
    "print(tagger.analyze('Bundestagspräsidentin',taglevel=3))\n",
    "print(tagger.analyze('Lehrerin',taglevel=3))\n",
    "print(tagger.analyze('Freundin',taglevel=3))\n",
    "print(tagger.analyze('Kollegin',taglevel=3))\n",
    "print(tagger.analyze('Lehrerinnen',taglevel=3))\n",
    "print(tagger.analyze('Freundinnen',taglevel=3))\n",
    "print(tagger.analyze('Genossinnen',taglevel=3))\n",
    "print(tagger.analyze('Ärztinnen',taglevel=3))\n",
    "print(tagger.analyze('FDP-Ehrenvorsitzende',taglevel=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('NN', -11.754250108561301)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagger.tag_word('Freundin')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate tagging and lemmatising on train data\n",
    "\n",
    "This is ofcourse not a proper evaluation as we use the training data to evaluate the program. However, if we don't get good results here, something went wrong for sure. The numbers we get here might give a kind of upperbound for what we can ecpect from a real evaluation.\n",
    "\n",
    "Evaluation of tagging and lemmatisation is not a trivial task. First, the evaluation data might have been tagged according to some othe tagging scheme. A lot of different decisions can be made about the tags and lemmata for many unclear cases. Moreover most data sets contain a large number of errors. Often these originate form the tagger/stemmer/lemmatiser that was used for the initial annotation and were overlooked in the manual correction phases.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(lines):\n",
    "    data = []\n",
    "    sent = []\n",
    "    lastsentnr = 1\n",
    "    for line in lines:\n",
    "        (sentnr,word,lemma,stem,tag) = line.split('\\t')[:5]\n",
    "        if sentnr != lastsentnr:\n",
    "            if len(sent) > 0:\n",
    "                data.append((sentnr,sent))\n",
    "            sent = []\n",
    "            lastsentnr = sentnr\n",
    "        sent.append((word,lemma,tag))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "\n",
    "datafile = codecs.open(r\"labeledmorph_ger.csv\", \"r\",\"utf-8\")\n",
    "\n",
    "morphdata = []\n",
    "for line in datafile:\n",
    "    if not line.startswith('-1'):\n",
    "        morphdata.append(line)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "testdata = load(morphdata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the POS Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_evaluate(sents):\n",
    "    correct = 0\n",
    "    nr = 0\n",
    "\n",
    "    for snr,sent in sents:\n",
    "        ws = [w for (w,l,c) in sent]\n",
    "        cs = [c for (w,l,c) in sent]\n",
    "        pred_cs = tagger.tag_sent(ws,taglevel = 0)\n",
    "        for i in range(len(ws)): \n",
    "            nr += 1\n",
    "            if cs[i] == pred_cs[i]:\n",
    "                correct += 1\n",
    "            #else:\n",
    "            #    c = cs[i].split('(')[0]\n",
    "            #    pred_c = pred_cs[i].split('(')[0]\n",
    "            #    if c != pred_c:\n",
    "            #        print(snr,'\\t',' '.join(ws))\n",
    "            #        print(ws[i],cs[i],pred_cs[i])\n",
    "            #        print()\n",
    "        if nr%50 == 0:\n",
    "            print(correct/nr,end='\\r')\n",
    "    return correct/nr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9824916849878799\r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9824790529996825"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_evaluate(testdata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate lemmatisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemma_evaluate(sents):\n",
    "    correct = 0\n",
    "    nr = 0\n",
    "\n",
    "    for snr,sent in sents:\n",
    "        ws = [w for (w,l,c) in sent]\n",
    "        ls = [l for (w,l,c) in sent]\n",
    "        pred_ls = [l for _,l,_ in tagger.tag_sent(ws,taglevel = 1)]\n",
    "        for i in range(len(ws)): \n",
    "            nr += 1\n",
    "            if ls[i] == pred_ls[i]:\n",
    "                correct += 1\n",
    "            #elif ls[i].lower() != pred_ls[i].lower():\n",
    "            #    print(snr,'\\t',' '.join(ws))\n",
    "            #    print(ws[i],ls[i],pred_ls[i])\n",
    "            #    print()\n",
    "        if nr%50 == 0:\n",
    "            print(correct/nr,end='\\r')\n",
    "    return correct/nr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9737053948926095\r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9737073206979556"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemma_evaluate(testdata)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
